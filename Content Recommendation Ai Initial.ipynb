{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_VkIyNMOtzak"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Multiply\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Processing\n",
        "class DataProcessor:\n",
        "    def __init__(self, content_data, user_data, interaction_data):\n",
        "        self.content_data = content_data\n",
        "        self.user_data = user_data\n",
        "        self.interaction_data = interaction_data\n",
        "        self.content_embeddings = None\n",
        "\n",
        "    def get_user_profile(self, user_id):\n",
        "        # Placeholder implementation\n",
        "        return np.random.rand(1, 100)  # Return a random vector as a user profile\n",
        "\n",
        "    def get_content_embeddings(self):\n",
        "        # Placeholder implementation\n",
        "        if self.content_embeddings is None:\n",
        "            self.content_embeddings = np.random.rand(len(self.content_data), 100)\n",
        "        return self.content_embeddings"
      ],
      "metadata": {
        "id": "wOdWdKMat1kW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Recommendation Models\n",
        "class HybridRecommender:\n",
        "    def __init__(self, n_factors=50, n_layers=3, reg_layers=[0, 0, 0], reg_mf=0):\n",
        "        self.n_factors = n_factors\n",
        "        self.n_layers = n_layers\n",
        "        self.reg_layers = reg_layers\n",
        "        self.reg_mf = reg_mf\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self, n_users, n_items):\n",
        "        user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
        "        item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
        "\n",
        "        MF_Embedding_User = Embedding(input_dim=n_users, output_dim=self.n_factors, name='mf_embedding_user',\n",
        "                                      embeddings_initializer='he_normal', embeddings_regularizer=l2(self.reg_mf),\n",
        "                                      input_length=1)\n",
        "        MF_Embedding_Item = Embedding(input_dim=n_items, output_dim=self.n_factors, name='mf_embedding_item',\n",
        "                                      embeddings_initializer='he_normal', embeddings_regularizer=l2(self.reg_mf),\n",
        "                                      input_length=1)\n",
        "\n",
        "        MLP_Embedding_User = Embedding(input_dim=n_users, output_dim=self.n_factors, name='mlp_embedding_user',\n",
        "                                       embeddings_initializer='he_normal', embeddings_regularizer=l2(self.reg_layers[0]),\n",
        "                                       input_length=1)\n",
        "        MLP_Embedding_Item = Embedding(input_dim=n_items, output_dim=self.n_factors, name='mlp_embedding_item',\n",
        "                                       embeddings_initializer='he_normal', embeddings_regularizer=l2(self.reg_layers[0]),\n",
        "                                       input_length=1)\n",
        "\n",
        "        mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "        mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "        mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
        "\n",
        "        mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "        mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "        mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "\n",
        "        for idx in range(1, self.n_layers):\n",
        "            layer = Dense(self.reg_layers[idx], activation='relu', name=f'layer_{idx}',\n",
        "                          kernel_regularizer=l2(self.reg_layers[idx]))\n",
        "            mlp_vector = layer(mlp_vector)\n",
        "\n",
        "        predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
        "        prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(predict_vector)\n",
        "\n",
        "        self.model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, user_id):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been built. Call build_model first.\")\n",
        "\n",
        "        try:\n",
        "            n_items = self.model.input[1].shape[1]  # Get the number of items from the model's input shape\n",
        "        except AttributeError:\n",
        "            print(\"Error: Model input shape is not as expected. Using a default value.\")\n",
        "            n_items = 1000  # Use a default value, adjust as needed\n",
        "\n",
        "        all_items = np.arange(n_items)\n",
        "        user_input = np.full(n_items, user_id)\n",
        "\n",
        "        try:\n",
        "            predictions = self.model.predict([user_input, all_items])\n",
        "            item_scores = [(item_id, score[0]) for item_id, score in zip(all_items, predictions)]\n",
        "            return sorted(item_scores, key=lambda x: x[1], reverse=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {e}\")\n",
        "            print(f\"User input shape: {user_input.shape}\")\n",
        "            print(f\"All items shape: {all_items.shape}\")\n",
        "            print(f\"Model input shapes: {[i.shape for i in self.model.inputs]}\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "vWWMKpO_t5sJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Training and Evaluation\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, data):\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "\n",
        "    def train(self, epochs=20, batch_size=256):\n",
        "        self.model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        history = self.model.fit([self.data.user_input, self.data.item_input], self.data.labels,\n",
        "                                 epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=2)\n",
        "        return history"
      ],
      "metadata": {
        "id": "_r9b_I7ut9BO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Real-time Update System\n",
        "class RealTimeUpdater:\n",
        "    def __init__(self, model, data_processor):\n",
        "        self.model = model\n",
        "        self.data_processor = data_processor\n",
        "\n",
        "    def update_user_preferences(self, user_id, interaction_data):\n",
        "        print(f\"Updating preferences for user {user_id}\")\n",
        "        # Placeholder: Update user preferences based on new interactions\n",
        "        pass\n",
        "\n",
        "    def update_content_features(self, new_content):\n",
        "        print(f\"Updating content features for {len(new_content)} new items\")\n",
        "        # Placeholder: Process and add new content to the system\n",
        "        pass\n",
        "\n",
        "    def adjust_recommendations(self, user_id, recommendations):\n",
        "        print(f\"Applying real-time adjustments for user {user_id}\")\n",
        "        # Placeholder: Apply any last-minute adjustments to recommendations\n",
        "        return recommendations\n"
      ],
      "metadata": {
        "id": "xrBpOV4Yt-0t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Main Recommendation Pipeline\n",
        "class RecommendationSystem:\n",
        "    def __init__(self, data_processor, recommender, trainer, real_time_updater):\n",
        "        self.data_processor = data_processor\n",
        "        self.recommender = recommender\n",
        "        self.trainer = trainer\n",
        "        self.real_time_updater = real_time_updater\n",
        "\n",
        "    def get_recommendations(self, user_id, top_n=10):\n",
        "        try:\n",
        "            cf_recs = self.recommender.predict(user_id)\n",
        "            cb_recs = self.content_based_recommendations(user_id)\n",
        "            combined_recs = self.combine_recommendations(cf_recs, cb_recs)\n",
        "            final_recs = self.real_time_updater.adjust_recommendations(user_id, combined_recs)\n",
        "            return final_recs[:top_n]\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_recommendations: {e}\")\n",
        "            return []\n",
        "\n",
        "    def content_based_recommendations(self, user_id):\n",
        "        user_profile = self.data_processor.get_user_profile(user_id)\n",
        "        content_embeddings = self.data_processor.get_content_embeddings()\n",
        "        content_similarities = cosine_similarity(user_profile, content_embeddings)\n",
        "        content_scores = content_similarities.flatten()\n",
        "        top_indices = content_scores.argsort()[::-1]\n",
        "        return [(i, content_scores[i]) for i in top_indices]  # Return index instead of content_id\n",
        "\n",
        "    def combine_recommendations(self, cf_recs, cb_recs, cf_weight=0.7, cb_weight=0.3):\n",
        "        combined = {}\n",
        "        for content_id, score in cf_recs:\n",
        "            combined[content_id] = score * cf_weight\n",
        "        for content_id, score in cb_recs:\n",
        "            if content_id in combined:\n",
        "                combined[content_id] += score * cb_weight\n",
        "            else:\n",
        "                combined[content_id] = score * cb_weight\n",
        "        return sorted(combined.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    def train_model(self, epochs=20, batch_size=256):\n",
        "        print(\"Starting model training...\")\n",
        "        history = self.trainer.train(epochs=epochs, batch_size=batch_size)\n",
        "        print(\"Model training completed.\")\n",
        "        return history\n",
        "\n",
        "    def update_system(self, new_data):\n",
        "        print(\"Updating system with new data...\")\n",
        "        self.real_time_updater.update_user_preferences(new_data['user_id'], new_data['interactions'])\n",
        "        self.real_time_updater.update_content_features(new_data['new_content'])\n",
        "        print(\"System update completed.\")\n",
        "\n",
        "    def evaluate_recommendations(self, test_data):\n",
        "        print(\"Evaluating recommendation quality...\")\n",
        "        # Placeholder: Implement evaluation metrics (e.g., precision, recall, NDCG)\n",
        "        pass"
      ],
      "metadata": {
        "id": "bVq9QPV0uBDo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize components with dummy data\n",
        "    n_users, n_items = 1000, 1000\n",
        "    dummy_content_data = pd.DataFrame({'content_id': range(n_items), 'title': [f'Item {i}' for i in range(n_items)]})\n",
        "    dummy_user_data = pd.DataFrame({'user_id': range(n_users)})\n",
        "    dummy_interaction_data = pd.DataFrame({'user_id': [], 'content_id': [], 'interaction': []})\n",
        "\n",
        "    data_processor = DataProcessor(dummy_content_data, dummy_user_data, dummy_interaction_data)\n",
        "    recommender = HybridRecommender()\n",
        "    model = recommender.build_model(n_users, n_items)\n",
        "    trainer = ModelTrainer(model, None)  # None instead of processed_data\n",
        "    real_time_updater = RealTimeUpdater(model, data_processor)\n",
        "\n",
        "    # Create recommendation system\n",
        "    rec_system = RecommendationSystem(data_processor, recommender, trainer, real_time_updater)\n",
        "\n",
        "    # Get recommendations for a user\n",
        "    user_id = 123\n",
        "    recommendations = rec_system.get_recommendations(user_id, top_n=10)\n",
        "    print(f\"Top 10 recommendations for user {user_id}:\")\n",
        "    for content_id, score in recommendations:\n",
        "        print(f\"Content ID: {content_id}, Score: {score}\")\n",
        "\n",
        "    # Update system with new data\n",
        "    new_data = {\n",
        "        'user_id': 123,\n",
        "        'interactions': [{'content_id': 456, 'interaction_type': 'view', 'duration': 300}],\n",
        "        'new_content': [{'content_id': 789, 'title': 'New Video', 'description': 'A brand new video'}]\n",
        "    }\n",
        "    rec_system.update_system(new_data)\n",
        "\n",
        "    # Note: We're skipping the actual training and evaluation steps in this example\n",
        "    # as they would require more complete data and model setup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5zMyLzKuD7-",
        "outputId": "1e527d22-bc18-46f3-8df4-1fe80890a5f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 166ms/step\n",
            "Applying real-time adjustments for user 123\n",
            "Top 10 recommendations for user 123:\n",
            "Content ID: 0, Score: 0.5772477276848731\n",
            "Content ID: 172, Score: 0.2527714495930758\n",
            "Content ID: 447, Score: 0.2506938701024575\n",
            "Content ID: 103, Score: 0.24975590759297636\n",
            "Content ID: 916, Score: 0.24873586429505953\n",
            "Content ID: 3, Score: 0.24857641644377532\n",
            "Content ID: 158, Score: 0.24814219313661456\n",
            "Content ID: 391, Score: 0.24789772511493313\n",
            "Content ID: 575, Score: 0.2474255239839387\n",
            "Content ID: 97, Score: 0.2471853904603532\n",
            "Updating system with new data...\n",
            "Updating preferences for user 123\n",
            "Updating content features for 1 new items\n",
            "System update completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VtXvYcE9uGVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}